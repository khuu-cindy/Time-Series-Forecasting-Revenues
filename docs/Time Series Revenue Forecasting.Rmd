---
title: "Revenue Forecasting with ARIMA, Prophet, & ETS/TBATS"
author: "Cindy Khuu"
credit source: "Business Science for Challenge"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = T,
    warning = T,
    paged.print = FALSE, 
    # This should allow Rmarkdown to locate the data
    root.dir = rprojroot::find_rstudio_root_file()
)
```

# Challenge Objective

Goal is to perform an 8-week revenue forecast using __ARIMA, Prophet, Exponential Smoothing, and TBATS models.__  Experiment with different parameter settings and feature engineering in time series forecasting. 


# Libraries

```{r, message=F, warning=F}
# Modeling
library(tidymodels)
library(modeltime)

# Core
library(tidyverse)
library(timetk)
library(lubridate)
```

## Import Artifacts 

First, import the `challenge_02_artifacts`.

```{r}
challenge_02_artifacts  <- read_rds("challenge_03_data/challenge_02_artifacts.rds")

```

Interactively, explore the artifacts, which includes the following: 
- raw data and process datasets with data at weekly level
- transformation parameters
- training/test splits for modeling
- workflow modeling recipes

```{r, eval=FALSE}
challenge_02_artifacts %>% View("artifacts")
```


## Data Preparation

Load the artifacts. 

```{r}
# Processed Data
data_prepared_tbl <- challenge_02_artifacts$processed_data$data_prepared_tbl
forecast_tbl      <- challenge_02_artifacts$processed_data$forecast_tbl

# Train/Test Splits
splits            <- challenge_02_artifacts$train_test_splits$splits

# Inversion Parameters
std_mean <- challenge_02_artifacts$transformation_params$standardize_params$std_mean
std_sd   <- challenge_02_artifacts$transformation_params$standardize_params$std_sd
```

# Train/Test Splits

Preview training data.

```{r}
training(splits)
```

Visualize the train/test split.

```{r}
splits %>% 
    tk_time_series_cv_plan() %>% 
    plot_time_series_cv_plan(.date_var = purchased_at,.value = revenue)
```


# Modeling 

There are 3 Sections in Modeling process:

- __ARIMA:__ Models 1-5
- __Prophet:__ Models 6-9
- __Exponential Smoothing & TBATS:__  Models 10-11

## ARIMA

We'll start by modeling the revenue with ARIMA. 

### Model 1 - Basic Auto ARIMA

```{r, message = T}
model_fit_1_arima_basic <- arima_reg() %>% 
    set_engine("auto_arima") %>% 
    fit(revenue ~ purchased_at, 
        data = training(splits))

model_fit_1_arima_basic
```

Observations: 
- Auto ARIMA produced orders of (0, 1, 1)
    - 0 lags were used
    - 1 differencing was performed
    - 1 lagged error features used?
- With this weekly dataset, auto-generate a frequency of 13 weeks per quarter
- This is a non-seasonal model since no seasonal terms were in the output

### Model 2 - Add Product Events

Next, I'll repeat the model but add events such as November sale and product launches:

Observations:
- The AIC improved from Model 1 of 183.41 to Model 2 of 128.99
- Coefficient in model add value and are not close to zero
- This is not a seasonal model

```{r, message = T}
model_fit_2_arima_xregs <- arima_reg() %>% 
    set_engine("auto_arima") %>% 
    fit(revenue ~ purchased_at + event_november_sale + event_product_launch, 
        data = training(splits))

model_fit_2_arima_xregs
```

### Model 3 - Add Seasonality + Product Events

After reviewing the ACF of the ARIMA model below are observations:
- ACF chart shows spikes in ACF Lags 4
- PACF chart shows spikes in PACF Lags 1 and 3

```{r, message = T}
training(splits) %>% 
    plot_acf_diagnostics(.date_var = purchased_at, .value = diff_vec(revenue))
```


On the 3rd Auto ARIMA model, I'll build upon the 2nd model and add season_period = 4 to try and capture the ACF Lag 4 as a seasonality

Observation:
- No this is still not a seasonal model after using auto arima

```{r}
model_fit_3_arima_sarimax <- arima_reg(seasonal_period = 4) %>% 
    set_engine("auto_arima") %>% 
    fit(revenue ~ purchased_at 
                + event_november_sale 
                + event_product_launch, 
        data = training(splits))

model_fit_3_arima_sarimax
```

### Model 4 - Force Seasonality w/ Regular ARIMA

Now, I'll force seasonality using ARIMA (not Auto-ARIMA) by adding monthly seasonality (4 weeks in a month)

Observations: 
- Now this is a seasonal model since we forced it.
- Adding seasonality did not appear to improve the model

```{r}
model_fit_4_arima_sarimax <- arima_reg(seasonal_period = 4, 
          # Non-seasonal terms
          non_seasonal_ar = 2, 
          non_seasonal_differences = 1, 
          non_seasonal_ma = 2, 
          # Seasonal terms
          seasonal_ar = 1, 
          seasonal_differences = 0, 
          seasonal_ma = 1) %>% 
    set_engine("arima") %>% 
    fit(revenue ~ purchased_at + 
                  event_november_sale + 
                  event_product_launch, 
        data = training(splits))

model_fit_4_arima_sarimax
```

### Model 5 - Use Fourier Terms + Product Events instead of Seasonality

Next, I'll try to improve the auto arima model by adding fourier series at period = 4 to capture strong ACF relationship at Lag 4.

Observations:
- The AIC did not improve much from Model 2 of 128.99, but it is close to it

```{r}
model_fit_5_arima_xreg_fourier <- arima_reg() %>% 
    set_engine("auto_arima") %>% 
    fit(revenue ~ purchased_at 
                + event_november_sale 
                + event_product_launch +     
                fourier_vec(purchased_at, period = 4), 
        data = training(splits))

model_fit_5_arima_xreg_fourier
```

### Investigate - Modeltime Workflow


#### Model Table

Consolidate Models 1 - 5 (ARIMA models)into a table.

```{r}
model_tbl_arima <- modeltime_table(
    model_fit_1_arima_basic,
    model_fit_2_arima_xregs,
    model_fit_3_arima_sarimax,
    model_fit_4_arima_sarimax,
    model_fit_5_arima_xreg_fourier
)

model_tbl_arima
```


#### Calibration Table

Calibrate models on the testing split. 

```{r}
calibration_tbl <- 
    model_tbl_arima %>% 
    modeltime_calibrate(testing(splits))

calibration_tbl
```

#### Test Accuracy

Calculate the test accuracy. 

Observations: 
- Best model so far is Model 5 with lower MAE and higher R-square compared to other models in the table

```{r}
calibration_tbl %>% 
    modeltime_accuracy()
```

#### Test Forecast

Forecast testing data using ARIMA models

```{r}
calibration_tbl %>% 
    modeltime_forecast(new_data = testing(splits), 
                   actual_data = data_prepared_tbl) %>% 
    plot_modeltime_forecast()
```

### ARIMA Forecast Review

- For the test data set (out of sample test), the ARIMA forecasts performed the best using Model 5 since it had the lowest MAE and explained most variance in data, which uses events and fourier series
- Global trend is overall increasing, but the models don't do a great job in capturing the global trend as most don't show upward trend
- Local trend on the most recent data a more steady and less steep upward trend, which seems to be captured my most models



## Prophet 

Next, I'll experiment with prophet algorithm.

### Model 6 - Basic Prophet 

```{r}
model_fit_6_prophet_basic <- prophet_reg() %>% 
    set_engine("prophet") %>% 
    fit(revenue ~ purchased_at, 
        data = training(splits))

model_fit_6_prophet_basic
```

### Model 7 - Turn on yearly seasonality

Next, I'll try turning yearly seasonality on.  

```{r}
model_fit_7_prophet_yearly <- prophet_reg(
    seasonality_yearly = TRUE
) %>% 
    set_engine("prophet") %>% 
    fit(revenue ~ purchased_at, 
        data = training(splits))

model_fit_7_prophet_yearly
```

### Model 8 - Product Events

Next, I'll remove forcing yearly seasonality, but add events. 

```{r}
model_fit_8_prophet_events <- prophet_reg(
) %>% 
    set_engine("prophet") %>% 
    fit(revenue ~ purchased_at 
        + event_november_sale
        + event_product_launch, 
        data = training(splits))

model_fit_8_prophet_events
```

### Model 9 - Events + Fourier Series

Next, I'll try another model that includes events and a fourier series similar to ARIMA models with period = 4 

```{r}
model_fit_9_prophet_events_fourier <- prophet_reg(
) %>% 
    set_engine("prophet") %>% 
    fit(revenue ~ purchased_at 
        + event_november_sale
        + event_product_launch
        + fourier_vec(purchased_at, period = 4), 
        data = training(splits))

model_fit_9_prophet_events_fourier
```

### Investigate - Modeltime Workflow


#### Model Table

Create a modeltime table with each of the prophet models 6-9 in the table

```{r}
model_tbl_prophet <- modeltime_table(
    model_fit_6_prophet_basic,
    model_fit_7_prophet_yearly,
    model_fit_8_prophet_events,
    model_fit_9_prophet_events_fourier
)

model_tbl_prophet
```


#### Calibration Table

Next, calibrate the models using testing data.

```{r}
calibration_tbl <- model_tbl_prophet %>% 
    modeltime_calibrate(testing(splits))

calibration_tbl

```

#### Test Accuracy

Calculate the accuracy. 

Observations: 
- Out of the prophet models, Model 9 has lowest MAE and explains the most variance (R-square) compared to other prophet models
- Model 9 (prophet model) performs close to Model 5 (ARIMA model), but ARIMA Model 5 is still better. In both models, we included fourier series and events. 

```{r}
calibration_tbl %>% 
    modeltime_accuracy()
```

#### Test Forecast

Finally, forecast the calibrated models for testing data. 

```{r}
calibration_tbl %>% 
    modeltime_forecast(new_data = testing(splits), 
                       actual_data = data_prepared_tbl) %>% 
    plot_modeltime_forecast()
```

### Prophet Forecast Review

- Model 8 and 9 had performed the best on forecasting test data. Model 9 was slightly better with lower MAE and highest explainable variance
- Most models were able to forecast the global trend (upward trend), but not the local trend (slightly flat or less steep)


## Exponential Smoothing

Next, I'll experiment incorporate exponential smoothing. 

### Model 10 - ETS

The first model I'll experiment with the automated "ETS" model:

Observations: 
- ETS parameters show (A, A, N), which means
    - Yes, this is an exponentially smoothed error model since first parameter is additive and there is an alpha parameter
    - Yes, this a trend based model since second parameter indicate additive and there's is a beta parameter
    - No, this not a seasonal model as third parameter is no. 

```{r}
model_fit_10_ets <- exp_smoothing() %>% 
    set_engine("ets") %>% 
    fit(revenue ~ purchased_at, 
        data = training(splits))

model_fit_10_ets
```



### Model 11 - TBATS

Next, I'll experiment with TBATS model with seasonality with `seasonal_period_1 = 4` due to ACF and `seasonal_period_2 = 13` for quarterly frequency. No external regressors can be added to this type of model.  

```{r}
model_fit_11_tbats <- seasonal_reg(
    seasonal_period_1 = 4, 
    seasonal_period_2 = 13
) %>% 
    set_engine("tbats") %>% 
    fit(revenue ~ purchased_at, 
        data = training(splits))

model_fit_11_tbats
```


### Investigate - Modeltime  Workflow


#### Model Table

Create a modeltime table with each of the exponential smoothing models 10-11 in the table.

```{r}
model_tbl_exp_smooth <- modeltime_table(
    model_fit_10_ets, 
    model_fit_11_tbats
) 

model_tbl_exp_smooth
```

#### Calibration Table

Next, calibrate the models using your testing set. 

```{r}
calibration_tbl <- model_tbl_exp_smooth %>% 
    modeltime_calibrate(new_data = testing(splits))

calibration_tbl
```

#### Test Accuracy

Calculate the accuracy. 

```{r}
calibration_tbl %>% 
    modeltime_accuracy()
```

#### Test Forecast

Finally, forecast the calibrated models on the testing data. 

```{r}
calibration_tbl %>% 
    modeltime_forecast(new_data = testing(splits), 
                       actual_data = data_prepared_tbl) %>% 
    plot_modeltime_forecast()
```
### Exponential Smoothing Forecast Review

- TBATS model appears to be better than ETS since it as a lower MAE, but it wasn't able to capture the variance
- Both exponential smoothing models perform below the ARIMA Model 5 and Prophet Model 9

# Forecast Future Data

Forecast the future timeframe.

### Model Table

Combine the 3 previous modeltime tables into a single modeltime table. Combine these Modeltime Tables: 
    - `model_tbl_arima`
    - `model_tbl_prophet`
    - `model_tbl_exp_smooth`
    
```{r}
model_tbl <- combine_modeltime_tables(
    model_tbl_arima,
    model_tbl_prophet,
    model_tbl_exp_smooth
)

model_tbl
```

```{r}
# Refitting makes sure models work over time. 
model_tbl <- model_tbl %>%
    modeltime_refit(training(splits))
```

### Calibrate the Table

Use testing data to calibrate the model

```{r}
calibration_tbl <- model_tbl %>% 
    modeltime_calibrate(testing(splits))
```

### Calculate the Accuracy

Calculate the accuracy metrics.

```{r}
calibration_tbl %>% 
    modeltime_accuracy()
```

### Visualize the Model Forecast


```{r}
calibration_tbl %>% 
    modeltime_forecast(new_data = testing(splits), 
                       actual_data = data_prepared_tbl) %>% 
    plot_modeltime_forecast()
```



### Refit

Refit models on full dataset.

```{r}
refit_tbl <- calibration_tbl %>% 
    modeltime_refit(data_prepared_tbl)
```

### Forecast Future 

Forecast using future horizon in `forecast_tbl`. 

```{r}
refit_tbl %>% 
    modeltime_forecast(new_data = forecast_tbl, 
                       actual_data = data_prepared_tbl) %>% 
    plot_modeltime_forecast()
```



## Invert Transformation

Apply the inversion to the forecast plot:

- Invert the standardization
- Invert the log transformation


```{r}
refit_tbl %>% 
    modeltime_forecast(new_data = forecast_tbl, 
                       actual_data = data_prepared_tbl) %>% 
    
    # Invert transformation
    mutate(across(.value:.conf_hi, .fns = ~ standardize_inv_vec(
        x = ., 
        mean = std_mean, 
        sd = std_sd
    ))) %>% 
    mutate(across(.value:.conf_hi, .fns = exp)) %>%

    # Plot forecast
    plot_modeltime_forecast()
```


# Forecast Review

- Overall, forecast are able to pick up either the global or local trend
- Best performing models are model 5 and model 9. Similarities between these were adding events and fourier series
